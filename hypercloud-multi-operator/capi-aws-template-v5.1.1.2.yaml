apiVersion: tmax.io/v1
categories:
- CAPI
imageUrl: https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png
kind: ClusterTemplate
metadata:
  name: capi-aws-template
objectKinds:
- Cluster
- AWSCluster
- KubeadmControlPlane
- AWSMachineTemplate
- MachineDeployment
- AWSMachineTemplate
- KubeadmConfigTemplate
objects:
- apiVersion: cluster.x-k8s.io/v1beta1
  kind: Cluster
  metadata:
    name: "${CLUSTER_NAME}"
    annotations:
      owner: ${OWNER}
  spec:
    clusterNetwork:
      pods:
        cidrBlocks: ["192.168.0.0/16"]
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      kind: AWSCluster
      name: "${CLUSTER_NAME}"
    controlPlaneRef:
      kind: KubeadmControlPlane
      apiVersion: controlplane.cluster.x-k8s.io/v1beta1
      name: "${CLUSTER_NAME}-control-plane"
- apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
  kind: AWSCluster
  metadata:
    name: "${CLUSTER_NAME}"
  spec:
    region: "${AWS_REGION}"
    sshKeyName: "${AWS_SSH_KEY_NAME}"
  bastion:
    enabled: false
    # enabled: true
- kind: KubeadmControlPlane
  apiVersion: controlplane.cluster.x-k8s.io/v1beta1
  metadata:
    name: "${CLUSTER_NAME}-control-plane"
  spec:
    replicas: ${CONTROL_PLANE_MACHINE_COUNT}
    machineTemplate:
      infrastructureRef:
        kind: AWSMachineTemplate
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        name: "${CLUSTER_NAME}-control-plane"
    kubeadmConfigSpec:
      initConfiguration:
        nodeRegistration:
          name: '{{ ds.meta_data.local_hostname }}'
          kubeletExtraArgs:
            cloud-provider: aws
      clusterConfiguration:
        apiServer:
          extraArgs:
            cloud-provider: aws
        controllerManager:
          extraArgs:
            cloud-provider: aws
      joinConfiguration:
        nodeRegistration:
          name: '{{ ds.meta_data.local_hostname }}'
          kubeletExtraArgs:
            cloud-provider: aws
      postKubeadmCommands:
      - mkdir -p $HOME/.kube
      - cp /etc/kubernetes/admin.conf $HOME/.kube/config
      - chown $USER:$USER $HOME/.kube/config
      - kubectl apply -f https://docs.projectcalico.org/archive/v3.16/manifests/calico.yaml
      - sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/g' /etc/kubernetes/manifests/kube-controller-manager.yaml || echo
      - sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/g' /etc/kubernetes/manifests/kube-scheduler.yaml || echo
      - sed -i "s/--listen-metrics-urls=http:\/\/127.0.0.1:2381/--listen-metrics-urls=http:\/\/127.0.0.1:2381,http:\/\/{{ ds.meta_data.local_ipv4 }}:2381/g" /etc/kubernetes/manifests/etcd.yaml || echo
    version: "${KUBERNETES_VERSION}"
- kind: AWSMachineTemplate
  apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
  metadata:
    name: "${CLUSTER_NAME}-control-plane"
  spec:
    template:
      spec:
        instanceType: "${AWS_CONTROL_PLANE_MACHINE_TYPE}"
        iamInstanceProfile: "control-plane.cluster-api-provider-aws.sigs.k8s.io"
        sshKeyName: "${AWS_SSH_KEY_NAME}"
        rootVolume:
          size: ${MASTER_DISK_SIZE}
- apiVersion: cluster.x-k8s.io/v1beta1
  kind: MachineDeployment
  metadata:
    name: "${CLUSTER_NAME}-md-0"
  spec:
    clusterName: "${CLUSTER_NAME}"
    replicas: ${WORKER_MACHINE_COUNT}
    selector:
      matchLabels:
    template:
      spec:
        clusterName: "${CLUSTER_NAME}"
        version: "${KUBERNETES_VERSION}"
        bootstrap:
          configRef:
            name: "${CLUSTER_NAME}-md-0"
            apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
            kind: KubeadmConfigTemplate
        infrastructureRef:
          name: "${CLUSTER_NAME}-md-0"
          apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
          kind: AWSMachineTemplate
- apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
  kind: AWSMachineTemplate
  metadata:
    name: "${CLUSTER_NAME}-md-0"
  spec:
    template:
      spec:
        instanceType: "${AWS_NODE_MACHINE_TYPE}"
        iamInstanceProfile: "nodes.cluster-api-provider-aws.sigs.k8s.io"
        sshKeyName: "${AWS_SSH_KEY_NAME}"
        rootVolume:
          size: ${WORKER_DISK_SIZE}
- apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
  kind: KubeadmConfigTemplate
  metadata:
    name: "${CLUSTER_NAME}-md-0"
  spec:
    template:
      spec:
        joinConfiguration:
          nodeRegistration:
            name: '{{ ds.meta_data.local_hostname }}'
            kubeletExtraArgs:
              cloud-provider: aws
parameters:
- description: namespace
  displayName: Namespace
  name: NAMESPACE
  required: false
  value: default
  valueType: string
- description: Cluster Owner
  displayName: Owner
  name: OWNER
  required: false
  value: admin
  valueType: string
- description: AWS REGION
  displayName: AWS region
  name: AWS_REGION
  required: false
  value: us-east-1
  valueType: string
- description: AWS SSH key name
  displayName: AWS SSH key name
  name: AWS_SSH_KEY_NAME
  required: false
  value: default
  valueType: string
- description: Cluster name
  displayName: ClusterName
  name: CLUSTER_NAME
  required: false
  value: clustername
  valueType: string
- description: Kubernetes version
  displayName: Kubernetes version
  name: KUBERNETES_VERSION
  required: false
  value: v1.18.2
  valueType: string
- description: Number of Master node
  displayName: number of master nodes
  name: CONTROL_PLANE_MACHINE_COUNT
  required: false
  value: 3
  valueType: number
- description: Master nodes instance type
  displayName: MasterNodeType
  name: AWS_CONTROL_PLANE_MACHINE_TYPE
  required: false
  value: t3.large
  valueType: string
- description: Number of Worker node
  displayName: number of worker nodes
  name: WORKER_MACHINE_COUNT
  required: false
  value: 3
  valueType: number
- description: Worker nodes instance type
  displayName: WorkerNodeType
  name: AWS_NODE_MACHINE_TYPE
  required: false
  value: t3.large
  valueType: string
- description: Master nodes disk type
  displayName: MasterDiskSize
  name: MASTER_DISK_SIZE
  required: false
  value: 20
  valueType: number
- description: Worker nodes disk type
  displayName: WorkerDiskSize
  name: WORKER_DISK_SIZE
  required: false
  value: 20
  valueType: number
recommend: true
shortDescription: Cluster template for CAPI provider AWS
urlDescription: ""

---

# k8s 1.19ìš© cluster template
apiVersion: tmax.io/v1
categories:
- CAPI
imageUrl: https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png
kind: ClusterTemplate
metadata:
  name: capi-aws-template-v1.19
objectKinds:
- Cluster
- AWSCluster
- KubeadmControlPlane
- AWSMachineTemplate
- MachineDeployment
- AWSMachineTemplate
- KubeadmConfigTemplate
objects:
- apiVersion: cluster.x-k8s.io/v1alpha3
  kind: Cluster
  metadata:
    annotations:
      federation: join
      owner: ${OWNER}
    name: ${CLUSTER_NAME}
    namespace: default
  spec:
    clusterNetwork:
      pods:
        cidrBlocks:
        - 192.168.0.0/16
    controlPlaneRef:
      apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
      kind: KubeadmControlPlane
      name: ${CLUSTER_NAME}-control-plane
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: AWSCluster
      name: ${CLUSTER_NAME}
- apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: AWSCluster
  metadata:
    name: ${CLUSTER_NAME}
    namespace: default
  spec:
    region: ${AWS_REGION}
    sshKeyName: ${AWS_SSH_KEY_NAME}
  bastion:
    enabled: false
    # enabled: true
- apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
  kind: KubeadmControlPlane
  metadata:
    name: ${CLUSTER_NAME}-control-plane
    namespace: default
  labels:
    cluster.tmax.io/cluster-name: '${CLUSTER_NAME}'
    cluster.tmax.io/controlplane: controlplane
  spec:
    infrastructureTemplate:
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: AWSMachineTemplate
      name: ${CLUSTER_NAME}-control-plane
    kubeadmConfigSpec:
      clusterConfiguration:
        apiServer:
          extraArgs:
            # audit-policy-file: /etc/kubernetes/pki/audit-policy.yaml
            # audit-webhook-config-file: /etc/kubernetes/pki/audit-webhook-config
            # audit-webhook-mode: batch
            cloud-provider: aws
            # oidc-ca-file: /etc/kubernetes/pki/hyperauth.crt
            # oidc-client-id: hypercloud5
            # oidc-groups-claim: group
            # oidc-issuer-url: ${HyperAuthUrl}
            # oidc-username-claim: preferred_username
            # oidc-username-prefix: '-'
        controllerManager:
          extraArgs:
            cloud-provider: aws
      # files:
      # - content: |
      #     ${HyperAuthCert}
      #   owner: root:root
      #   path: /etc/kubernetes/pki/hyperauth.crt
      #   permissions: "0644"
      # - content: |
      #     apiVersion: v1
      #     kind: Config
      #     clusters:
      #     - cluster:
      #         certificate-authority-data: ${AUDIT_WEBHOOK_CA_CERT}
      #         server: ${AUDIT_WEBHOOK_SERVER_PATH}
      #       name: audit-webhook-service
      #     contexts:
      #     - context:
      #         cluster: audit-webhook-service
      #       name: audit-webhook-context
      #     current-context: audit-webhook-context
      #   owner: root:root
      #   path: /etc/kubernetes/pki/audit-webhook-config
      #   permissions: "0644"
      # - content: |
      #     apiVersion: audit.k8s.io/v1 # This is required.
      #     kind: Policy
      #     omitStages:
      #       - "ResponseStarted"
      #       - "RequestReceived"
      #     rules:

      #       # Don't log system requests.
      #       - level: None
      #         userGroups: ["system:serviceaccounts:hypercloud5-system", "system:nodes", "system:serviceaccounts", "system:masters"]
              
      #       # Don't log requests by hypercloud5-admin and kube-system user.
      #       - level: None
      #         users: ["system:serviceaccount:hypercloud5-system:hypercloud5-admin", "system:kube-controller-manager", "system:kube-scheduler", "system:apiserver"]
              
      #       # Don't log watch and get request
      #       - level: None
      #         verbs: ["watch", "get", "list"]
              
      #       # Log the request body of configmap changes in kube-system.
      #       - level: Metadata
      #         resources:
      #         # k8s resource
      #         - group: ""
      #         - group: "admissionregistration.k8s.io"
      #         - group: "apiextensions.k8s.io"
      #         - group: "apiregistration.k8s.io"
      #         - group: "apps"
      #         - group: "autoscaling"
      #         - group: "rbac.authorization.k8s.io"
      #         - group: "batch"
      #         - group: "servicecatalog.k8s.io"
      #         - group: "storage.k8s.io"
      #         - group: "policy"
      #         # storage
      #         - group: "ceph.rook.io"
      #         # istio
      #         - group: "authentication.istio.io"
      #         - group: "config.istio.io"
      #         - group: "networking.istio.io"
      #         - group: "rbac.istio.io"
      #         - group: "security.istio.io"
      #         # tmax.io
      #         - group: "tmax.io"
      #         - group: "claim.tmax.io"
      #         - group: "cluster.tmax.io"
      #         - group: "console.tmax.io"
      #         - group: "cicd.tmax.io"
      #         - group: "cicdapi.tmax.io"
      #         - group: "helmapi.tmax.io"
      #         # cicd
      #         - group: "tekton.dev"
      #         - group: "triggers.tekton.dev"
      #         # multi cluster
      #         - group: "types.kubefed.io"
      #         - group: "core.kubefed.io"
      #         - group: "cluster.x-k8s.io"
      #         - group: "addons.cluster.x-k8s.io"
      #         - group: "exp.cluster.x-k8s.io"
      #         - group: "bootstrap.cluster.x-k8s.io"
      #         - group: "controlplane.cluster.x-k8s.io"
      #         - group: "infrastructure.cluster.x-k8s.io"
      #         - group: "multiclusterdns.kubefed.io"
      #         - group: "scheduling.kubefed.io"
      #         # hyperframe
      #         - group: "kafka.strimzi.io"
      #         - group: "redis.redis.opstreelabs.in"
      #         # ML
      #         - group: "kubeflow.org"
      #         - group: "serving.kubeflow.org"
      #         # etc
      #         - group: "binding.operators.coreos.com"
            
      #       # Don't log others.
      #       - level: None
      #   owner: root:root
      #   path: /etc/kubernetes/pki/audit-policy.yaml
      #   permissions: "0644"
      initConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: aws
          name: '{{ ds.meta_data.local_hostname }}'
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: aws
          name: '{{ ds.meta_data.local_hostname }}'
      postKubeadmCommands:
      - mkdir -p $HOME/.kube
      - cp /etc/kubernetes/admin.conf $HOME/.kube/config
      - chown $USER:$USER $HOME/.kube/config
      - kubectl apply -f https://docs.projectcalico.org/archive/v3.16/manifests/calico.yaml
      - sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/g' /etc/kubernetes/manifests/kube-controller-manager.yaml || echo
      - sed -i 's/--bind-address=127.0.0.1/--bind-address=0.0.0.0/g' /etc/kubernetes/manifests/kube-scheduler.yaml || echo
      - sed -i "s/--listen-metrics-urls=http:\/\/127.0.0.1:2381/--listen-metrics-urls=http:\/\/127.0.0.1:2381,http:\/\/{{ ds.meta_data.local_ipv4 }}:2381/g" /etc/kubernetes/manifests/etcd.yaml || echo
    replicas: ${CONTROL_PLANE_MACHINE_COUNT}
    version: ${KUBERNETES_VERSION}
- apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: AWSMachineTemplate
  metadata:
    name: ${CLUSTER_NAME}-control-plane
    namespace: default
  spec:
    template:
      spec:
        iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io
        instanceType: ${AWS_CONTROL_PLANE_MACHINE_TYPE}
        sshKeyName: ${AWS_SSH_KEY_NAME}
        rootVolume:
          size: ${MASTER_DISK_SIZE}
- apiVersion: cluster.x-k8s.io/v1alpha3
  kind: MachineDeployment
  metadata:
    name: ${CLUSTER_NAME}-md-0
    namespace: default
    labels:
      cluster.tmax.io/cluster-name: '${CLUSTER_NAME}'
      cluster.tmax.io/worker: worker
  spec:
    clusterName: ${CLUSTER_NAME}
    replicas: ${WORKER_MACHINE_COUNT}
    selector: {}
    template:
      spec:
        bootstrap:
          configRef:
            apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
            kind: KubeadmConfigTemplate
            name: ${CLUSTER_NAME}-md-0
        clusterName: ${CLUSTER_NAME}
        infrastructureRef:
          apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
          kind: AWSMachineTemplate
          name: ${CLUSTER_NAME}-md-0
        version: ${KUBERNETES_VERSION}
- apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: AWSMachineTemplate
  metadata:
    name: ${CLUSTER_NAME}-md-0
    namespace: default
  spec:
    template:
      spec:
        iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io
        instanceType: ${AWS_NODE_MACHINE_TYPE}
        sshKeyName: ${AWS_SSH_KEY_NAME}
        rootVolume:
          size: ${WORKER_DISK_SIZE}
- apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
  kind: KubeadmConfigTemplate
  metadata:
    name: ${CLUSTER_NAME}-md-0
    namespace: default
  spec:
    template:
      spec:
        clusterConfiguration:
          apiServer:
            extraArgs:
              audit-webhook-mode: batch
        joinConfiguration:
          nodeRegistration:
            kubeletExtraArgs:
              cloud-provider: aws
            name: '{{ ds.meta_data.local_hostname }}'
parameters:
- description: namespace
  displayName: Namespace
  name: NAMESPACE
  required: false
  value: default
  valueType: string
- description: Cluster Owner
  displayName: Owner
  name: OWNER
  required: false
  value: admin
  valueType: string
- description: AWS REGION
  displayName: AWS region
  name: AWS_REGION
  required: false
  value: us-east-1
  valueType: string
- description: AWS SSH key name
  displayName: AWS SSH key name
  name: AWS_SSH_KEY_NAME
  required: false
  value: default
  valueType: string
- description: Cluster name
  displayName: ClusterName
  name: CLUSTER_NAME
  required: false
  value: clustername
  valueType: string
- description: Kubernetes version
  displayName: Kubernetes version
  name: KUBERNETES_VERSION
  required: false
  value: v1.18.2
  valueType: string
- description: Number of Master node
  displayName: number of master nodes
  name: CONTROL_PLANE_MACHINE_COUNT
  required: false
  value: 3
  valueType: number
- description: Master nodes instance type
  displayName: MasterNodeType
  name: AWS_CONTROL_PLANE_MACHINE_TYPE
  required: false
  value: t3.large
  valueType: string
- description: Number of Worker node
  displayName: number of worker nodes
  name: WORKER_MACHINE_COUNT
  required: false
  value: 3
  valueType: number
- description: Worker nodes instance type
  displayName: WorkerNodeType
  name: AWS_NODE_MACHINE_TYPE
  required: false
  value: t3.large
  valueType: string
- description: Master nodes disk type
  displayName: MasterDiskSize
  name: MASTER_DISK_SIZE
  required: false
  value: 20
  valueType: number
- description: Worker nodes disk type
  displayName: WorkerDiskSize
  name: WORKER_DISK_SIZE
  required: false
  value: 20
#   valueType: number
# - description: HyperAuth url for open id connect
#   displayName: HyperAuth URL
#   name: HyperAuthUrl
#   required: false
#   value: hyperauth.tmax.co.kr
#   valueType: string
# - description: HyperAuth tls Certifcate
#   displayName: HyperAuth Cert
#   name: HyperAuthCert
#   required: false
#   value: xxxx
#   valueType: string
recommend: true
shortDescription: Cluster template for CAPI provider AWS
urlDescription: ""
